import Admonition from "@theme/Admonition";

# Embeddings

### Amazon Bedrock Embeddings

Used to load [Amazon Bedrocks’s](https://aws.amazon.com/bedrock/) embedding models.

**Params**

- **credentials_profile_name:** The name of the profile in the ~/.aws/credentials or ~/.aws/config files, which has either access keys or role information specified. If not specified, the default credential profile or, if on an EC2 instance, credentials from IMDS will be used. See [the AWS documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html) for more details.

- **model_id:** Id of the model to call, e.g., amazon.titan-embed-text-v1, this is equivalent to the modelId property in the list-foundation-models api.

- **endpoint_url:** Needed if you don’t want to default to us-east-1 endpoint.

- **region_name:** The aws region e.g., us-west-2. Fallsback to AWS_DEFAULT_REGION env variable or region specified in ~/.aws/config in case it is not provided here.

---

### Cohere Embeddings

Used to load [Cohere’s](https://cohere.com/) embedding models.

**Params**

- **cohere_api_key:** Holds the API key required to authenticate with the Cohere service.

- **model:** The language model used for embedding text documents and performing queries —defaults to `embed-english-v2.0`.

- **truncate:** Used to specify whether or not to truncate the input text. Truncation is useful when dealing with long texts that exceed the model's maximum input length. By truncating the text, the user can ensure that it fits within the model's constraints.

---

### Azure OpenAI Embeddings

Generate embeddings using Azure OpenAI models.

**Params**

- **Azure Endpoint:** Your Azure endpoint, including the resource. Example: `https://example-resource.azure.openai.com/`
- **Deployment Name:** The name of the deployment.
- **API Version:** The API version to use. (Options: 2022-12-01, 2023-03-15-preview, 2023-05-15, 2023-06-01-preview, 2023-07-01-preview, 2023-08-01-preview)
- **API Key:** The API key to access the Azure OpenAI service.

---

### Hugging Face API Embeddings

Generate embeddings using Hugging Face Inference API models.

**Params**

- **API Key:** API key for accessing the Hugging Face Inference API. (Type: str)
- **API URL:** URL of the Hugging Face Inference API. (Default: http://localhost:8080)
- **Model Name:** Name of the model to use. (Default: BAAI/bge-large-en-v1.5)
- **Cache Folder:** Folder path to cache Hugging Face models. (Advanced)
- **Encode Kwargs:** Additional arguments for the encoding process. (Type: dict, Advanced)
- **Model Kwargs:** Additional arguments for the model. (Type: dict, Advanced)
- **Multi Process:** Whether to use multiple processes. (Default: False, Advanced)

---

### Hugging Face Embeddings

Used to load [HuggingFace’s](https://huggingface.co) embedding models.

**Params**

- **Cache Folder:** Folder path to cache HuggingFace models.
- **Encode Kwargs:** Additional arguments for the encoding process. (Type: dict)
- **Model Kwargs:** Additional arguments for the model. (Type: dict)
- **Model Name:** Name of the HuggingFace model to use. (Default: sentence-transformers/all-mpnet-base-v2)
- **Multi Process:** Whether to use multiple processes. (Default: False)

---

### Ollama Embeddings

Generate embeddings using Ollama models.

**Params**

- **Ollama Model:** Name of the Ollama model to use. (Default: llama2)
- **Ollama Base URL:** Base URL of the Ollama API. (Default: http://localhost:11434)
- **Model Temperature:** Temperature parameter for the model. (Type: float)

---

### OpenAI Embeddings

Used to load [OpenAI’s](https://openai.com/) embedding models.

**Params**

- **OpenAI API Key:** The API key to use for accessing the OpenAI API. (Type: str)
- **Default Headers:** Default headers for the HTTP requests. (Type: Dict[str, str], Optional)
- **Default Query:** Default query parameters for the HTTP requests. (Type: NestedDict, Optional)
- **Allowed Special:** Special tokens allowed for processing. (Type: List[str], Default: [])
- **Disallowed Special:** Special tokens disallowed for processing. (Type: List[str], Default: ["all"])
- **Chunk Size:** Chunk size for processing. (Type: int, Default: 1000)
- **Client:** HTTP client for making requests. (Type: Any, Optional)
- **Deployment:** Deployment name for the model. (Type: str, Default: "text-embedding-3-small")
- **Embedding Context Length:** Length of embedding context. (Type: int, Default: 8191)
- **Max Retries:** Maximum number of retries for failed requests. (Type: int, Default: 6)
- **Model:** Name of the model to use. (Type: str, Default: "text-embedding-3-small")
- **Model Kwargs:** Additional keyword arguments for the model. (Type: NestedDict, Optional)
- **OpenAI API Base:** Base URL of the OpenAI API. (Type: str, Optional)
- **OpenAI API Type:** Type of the OpenAI API. (Type: str, Optional)
- **OpenAI API Version:** Version of the OpenAI API. (Type: str, Optional)
- **OpenAI Organization:** Organization associated with the API key. (Type: str, Optional)
- **OpenAI Proxy:** Proxy server for the requests. (Type: str, Optional)
- **Request Timeout:** Timeout for the HTTP requests. (Type: float, Optional)
- **Show Progress Bar:** Whether to show a progress bar for processing. (Type: bool, Default: False)
- **Skip Empty:** Whether to skip empty inputs. (Type: bool, Default: False)
- **TikToken Enable:** Whether to enable TikToken. (Type: bool, Default: True)
- **TikToken Model Name:** Name of the TikToken model. (Type: str, Optional)

---

### VertexAI Embeddings

Wrapper around [Google Vertex AI](https://cloud.google.com/vertex-ai) [Embeddings API](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings).

<Admonition type="info">
  Vertex AI is a cloud computing platform offered by Google Cloud Platform
  (GCP). It provides access, management, and development of applications and
  services through global data centers. To use Vertex AI PaLM, you need to have
  the
  [google-cloud-aiplatform](https://pypi.org/project/google-cloud-aiplatform/)
  Python package installed and credentials configured for your environment.
</Admonition>

- **credentials:** The default custom credentials (google.auth.credentials.Credentials) to use.
- **location:** The default location to use when making API calls – defaults to `us-central1`.
- **max_output_tokens:** Token limit determines the maximum amount of text output from one prompt – defaults to `128`.
- **model_name:** The name of the Vertex AI large language model – defaults to `text-bison`.
- **project:** The default GCP project to use when making Vertex API calls.
- **request_parallelism:** The amount of parallelism allowed for requests issued to VertexAI models – defaults to `5`.
- **temperature:** Tunes the degree of randomness in text generations. Should be a non-negative value – defaults to `0`.
- **top_k:** How the model selects tokens for output, the next token is selected from – defaults to `40`.
- **top_p:** Tokens are selected from most probable to least until the sum of their – defaults to `0.95`.
- **tuned_model_name:** The name of a tuned model. If provided, model_name is ignored.
- **verbose:** This parameter is used to control the level of detail in the output of the chain. When set to True, it will print out some internal states of the chain while it is being run, which can help debug and understand the chain's behavior. If set to False, it will suppress the verbose output – defaults to `False`.
